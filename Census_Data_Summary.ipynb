{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and import tahoe geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arcpy\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "service_url = 'https://maps.trpa.org/server/rest/services/Demographics/FeatureServer/27'\n",
    "\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "tahoe_geometry_fields = ['YEAR', 'STATE', 'GEOGRAPHY', 'GEOID', 'TRPAID', 'NEIGHBORHOOD']\n",
    "query_result = feature_layer.query(out_fields=\",\".join(tahoe_geometry_fields))\n",
    "# Convert the query result to a list of dictionaries\n",
    "feature_list = query_result.features\n",
    "\n",
    "# Create a pandas DataFrame from the list of dictionaries\n",
    "tahoe_geometry = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "\n",
    "county_lookup = {\n",
    "    '005': 'Douglas County (Tahoe Basin)',\n",
    "    '017': 'El Dorado County (Tahoe Basin)',\n",
    "    '031': 'Washoe County (Tahoe Basin)',\n",
    "    '061': 'Placer County (Tahoe Basin)'\n",
    "}\n",
    "\n",
    "\n",
    "inflation_adjustment = {\n",
    "    '2000': 1.57,\n",
    "    '2010': 1.24\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_append_df(df, summary_df):\n",
    "    if df.empty:\n",
    "        df = summary_df.copy()\n",
    "    else:\n",
    "        df = pd.concat([df, summary_df])\n",
    "    return df\n",
    "\n",
    "def calculate_median_value(df, bin_column, sort_column, count_column, category_field, category,grouping_variables, cumulative_sorting_variables):\n",
    "        # Create a new DataFrame to avoid modifying the original one\n",
    "    #change value to count column\n",
    "    #Do we need to handle excluding non-tahoe things here or should we do it in the input?\n",
    "    summary_df = df.copy()\n",
    "    summary_df[count_column]=summary_df[count_column].astype(int)\n",
    "    #summary_df=summary_df.loc[summary_df[count_column]!='510']\n",
    "    summary_df = summary_df.groupby(grouping_variables)[count_column].sum()\n",
    "    summary_df = summary_df.reset_index()\n",
    "    #This handles -6666 values that they sometimes add for unknown data\n",
    "    summary_df = summary_df.loc[summary_df[count_column]>=0]\n",
    "    \n",
    "    \n",
    "    summary_df= summary_df.loc[summary_df[category_field]==category]\n",
    "    # Sort the DataFrame based on the variable name column\n",
    "    # This depends on the fact that census variables start with the lowest value and go up \n",
    "    #This needs to all be rethought to have some kind of window function to handle multiple years\n",
    "    summary_df.sort_values(by=sort_column, inplace=True)\n",
    "    summary_df = summary_df.reset_index()\n",
    "    \n",
    "    # Extract lower and upper limits from bin categories\n",
    "    #This uses regex to find numbers and removes commas to make numbers numbers \n",
    "    pattern = r'(\\d+[\\d,]*)'\n",
    "    summary_df['temp'] = summary_df[bin_column].str.replace(',', '').str.findall(pattern)\n",
    "    #Looks for values that have two numbers and puts empty placeholders for the ones that only have one (upper and lower)\n",
    "    summary_df[['Lower', 'Upper']] = summary_df['temp'].apply(lambda x: pd.Series(x[:2]) if len(x) == 2 else pd.Series([None, None]))\n",
    "    summary_df['Lower'] = summary_df['Lower'].astype(float)\n",
    "    summary_df['Upper'] = summary_df['Upper'].astype(float)\n",
    "    # Handle first category\n",
    "    \n",
    "    first_upper = float(summary_df['temp'].iloc[0][0])\n",
    "    low_variable_name = summary_df[bin_column].iloc[0]\n",
    "\n",
    "    summary_df.loc[summary_df[bin_column]==low_variable_name,'Lower'] = 0  # Set lower value to 0\n",
    "    summary_df.loc[summary_df[bin_column]==low_variable_name,'Upper'] = first_upper\n",
    "\n",
    "    \n",
    "    # Handle last category\n",
    "    \n",
    "    last_lower = float(summary_df['temp'].iloc[-1][0])\n",
    "    upper_variable_name = summary_df[bin_column].iloc[-1]\n",
    "    summary_df.loc[summary_df[bin_column]==upper_variable_name,'Lower'] = last_lower\n",
    "    summary_df.loc[summary_df[bin_column]==upper_variable_name,'Upper'] = np.inf  # Set upper value to infinity\n",
    "    summary_df[count_column]= summary_df[count_column].astype(float)   \n",
    "    # Calculate cumulative count\n",
    "    cumulative_grouping_variables = grouping_variables\n",
    "\n",
    "    cumulative_grouping_variables.remove(bin_column)\n",
    "    cumulative_grouping_variables.remove(sort_column)\n",
    "\n",
    "    #Update this to be parameterized\n",
    "    \n",
    "    summary_df.sort_values(by=cumulative_sorting_variables, inplace=True)\n",
    "    summary_df = summary_df.reset_index()\n",
    "    summary_df['cumulative_sum'] = summary_df.groupby(cumulative_grouping_variables, as_index=False)[count_column].cumsum()\n",
    "    summary_df['TotalSum'] = summary_df.groupby(cumulative_grouping_variables, as_index=False)[count_column].transform('sum')\n",
    "    summary_df['previous_cumulative'] = summary_df['cumulative_sum'].shift()\n",
    "\n",
    "    summary_df = summary_df.loc[summary_df['cumulative_sum']>=(summary_df['TotalSum']/2)].groupby(cumulative_grouping_variables, as_index=False).first()\n",
    "\n",
    "    summary_df['cumulative_difference'] = summary_df['TotalSum']  / 2 - summary_df['previous_cumulative']\n",
    "    summary_df['interpolation_ratio'] = summary_df['cumulative_difference'] /  (summary_df['cumulative_sum']- summary_df['previous_cumulative'])\n",
    "    summary_df['median_value'] = summary_df['Lower'] + summary_df['interpolation_ratio'] * (summary_df['Upper'] - summary_df['Lower'])\n",
    "    \n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "def categorize_values(census_df, category_csv, category_column, grouping_prefix):\n",
    "    categories = pd.read_csv(category_csv)    \n",
    "    census_df['value'] = census_df['value'].astype(float)\n",
    "    joined_data = census_df.merge(categories, on = 'variable_code', how = 'left')\n",
    "    joined_data.sort_values(by='variable_code', inplace=True)\n",
    "    #This will get rid of any extra columns in the category_csv\n",
    "    group_columns = [column for column in census_df if column not in ['value', 'variable_code', 'variable_name', 'MarginOfError','OBJECTID']]\n",
    "    group_columns.append(category_column)\n",
    "    #grouped_data = joined_data.groupby(group_columns, as_index=False)['value'].sum()    \n",
    "    print(group_columns)\n",
    "    grouped_data = joined_data.groupby(group_columns, as_index=False, dropna=False).agg({'value':'sum',\n",
    "                                                                           'variable_code':lambda x: grouping_prefix +  ', '.join(x)})\n",
    "    \n",
    "    #Need to return this formatted for appending to the table - need to get locations of variable_code and variable name, \n",
    "    #add them in as columns in those locations and then populate them with category column nanme\n",
    "    var_code_col_location = census_df.columns.get_loc('variable_code')\n",
    "    var_name_col_location = census_df.columns.get_loc('variable_name')\n",
    "    var_moe_col_location = census_df.columns.get_loc('MarginOfError')\n",
    "    grouped_data.insert(var_moe_col_location, 'MarginOfError', '')\n",
    "    #grouped_data.insert(var_code_col_location, 'variable_code','Grouped Value')\n",
    "    grouped_data.insert(var_name_col_location, 'variable_name','')\n",
    "    #grouped_data['variable_code'] = grouped_data['variable_code'] +  '_Grouped'\n",
    "    grouped_data['variable_name'] = grouped_data[category_column]\n",
    "    grouped_data['dataset']= grouping_prefix + grouped_data['dataset']\n",
    "    grouped_data['variable_category']= grouping_prefix +  grouped_data['variable_category'] \n",
    "    columns_to_keep = [column for column in census_df if column not in ['OBJECTID']]\n",
    "    grouped_data= grouped_data[columns_to_keep]\n",
    "    return grouped_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Basin Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data for Basin Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_36192\\1668274330.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data_clean.loc[all_data_clean['county'].isin(['005','017']),'north_south'] = 'South Lake'\n",
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_36192\\1668274330.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data_clean['county_name'] = all_data_clean['county'].apply(lambda x: county_lookup.get(x, None))\n"
     ]
    }
   ],
   "source": [
    "service_url = 'https://maps.trpa.org/server/rest/services/Demographics/FeatureServer/28'\n",
    "\n",
    "feature_layer = FeatureLayer(service_url)\n",
    "query_result = feature_layer.query()\n",
    "# Convert the query result to a list of dictionaries\n",
    "feature_list = query_result.features\n",
    "\n",
    "# Create a pandas DataFrame from the list of dictionaries\n",
    "all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "\n",
    "all_data_clean = all_data.loc[(all_data['county']!='510')&(all_data['value']>=0)]\n",
    "all_data_clean.loc[all_data_clean['county'].isin(['005','017']),'north_south'] = 'South Lake'\n",
    "all_data_clean.loc[all_data_clean['county'].isin(['031','061']),'north_south'] = 'North Lake'\n",
    "all_data_clean['county_name'] = all_data_clean['county'].apply(lambda x: county_lookup.get(x, None))\n",
    "\n",
    "all_data_clean = all_data_clean.dropna(subset='value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summed Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population Totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_population_data = all_data_clean.loc[(all_data_clean['variable_name']=='Total Population')]\n",
    "filtered_population_data\n",
    "summarized_population_data_basin = filtered_population_data.groupby([ 'dataset', 'variable_code', 'year_sample'], as_index=False).sum(['value'])\n",
    "summarized_population_data_county = filtered_population_data.groupby(['dataset', 'variable_code', 'year_sample', 'county_name'], as_index=False).sum(['value'])\n",
    "summarized_population_data_north_south = filtered_population_data.groupby(['dataset', 'variable_code', 'year_sample', 'north_south'], as_index=False).sum(['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_across_levels(df, variable_name, category_name):\n",
    "    filtered_df = df.loc[(df['variable_name']==variable_name)]\n",
    "    basin_summary = filtered_df.groupby([ 'dataset', 'sample_level', 'variable_name', 'variable_code', 'year_sample'], as_index=False).sum(['value'])\n",
    "    county_summary = filtered_df.groupby(['dataset', 'sample_level', 'variable_name', 'variable_code', 'year_sample', 'county_name'], as_index=False).sum(['value'])\n",
    "    north_south_summary = filtered_df.groupby(['dataset', 'sample_level', 'variable_name', 'variable_code', 'year_sample', 'north_south'], as_index=False).sum(['value'])\n",
    "    #basin_summary.rename(columns = {'variable_code': 'Code', 'year_sample': 'Year'})\n",
    "    basin_summary['Geography'] = 'Basin'\n",
    "    county_summary['Geography'] = county_summary['county_name']\n",
    "    north_south_summary['Geography'] = north_south_summary['north_south']\n",
    "    columns_to_keep = ['variable_code','variable_name', 'value', 'Geography', 'year_sample', 'dataset', 'sample_level']\n",
    "    basin_summary= basin_summary[columns_to_keep]\n",
    "    county_summary = county_summary[columns_to_keep]\n",
    "    north_south_summary = north_south_summary[columns_to_keep]\n",
    "    combined_summary = pd.concat([basin_summary, county_summary, north_south_summary], ignore_index=True)\n",
    "    #if neighborhood_yn == 'Yes':\n",
    "    #    neighborhood_summary = filtered_df.groupby(['dataset', 'sample_level', 'variable_name', 'variable_code', 'year_sample', 'NEIGHBORHOOD'], as_index=False).sum(['value'])\n",
    "        #basin_summary.rename(columns = {'variable_code': 'Code', 'year_sample': 'Year'})\n",
    "    #    neighborhood_summary['Geography'] = neighborhood_summary['NEIGHBORHOOD']\n",
    "    #    combined_summary = pd.concat([combined_summary, neighborhood_summary], ignore_index=True)\n",
    "    combined_summary['Category'] = category_name\n",
    "    return combined_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_income_data = all_data_clean.loc[(all_data_clean['variable_category']=='Household Income')]\n",
    "variable_name = '$200,000 or more'\n",
    "category_name = 'Household Income'\n",
    "rich_person_summary = sum_across_levels(all_data_clean, variable_name, category_name)\n",
    "rich_person_summary.to_excel('RichPeople.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_name = 'Total Population'\n",
    "category_name = 'Total Population'\n",
    "dataset_name = 'Test'\n",
    "\n",
    "total_population_summary = sum_across_levels(all_data_clean, variable_name, category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_housing_data = all_data_clean.loc[(all_data_clean['sample_level']!='block')]\n",
    "variable_name = 'Total Housing Units'\n",
    "category_name = 'Housing Units'\n",
    "\n",
    "total_housing_units_summary = sum_across_levels(filtered_housing_data, variable_name, category_name)\n",
    "total_housing_units_summary.to_excel('total_housing_units_summary.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_housing_data = all_data_clean.loc[(all_data_clean['sample_level']!='block')]\n",
    "#variable_name = 'Total Housing Units'\n",
    "category_name = 'Housing Units'\n",
    "\n",
    "variable_list= ['Total Housing Units', 'Total Housing Units: Occupied', \n",
    "    'Total Housing Units: Vacant', 'Vacant Housing Units: Seasonal, recreational, or occasional use',\n",
    "    'Occupied Housing Units: Owner Occupied', 'Occupied Housing Units: Renter Occupied']\n",
    "housing_unit_summary=pd.DataFrame()\n",
    "for variable in variable_list:\n",
    "    df = sum_across_levels(filtered_housing_data, variable, category_name)\n",
    "    housing_unit_summary = create_or_append_df(housing_unit_summary, df)\n",
    "\n",
    "housing_unit_summary.to_excel('housing_unit_summaries.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_variables = ['variable_category', 'year_sample', 'dataset', 'variable_code', 'sample_level', 'variable_name']\n",
    "\n",
    "clean_income_data= all_data_clean.loc[(all_data_clean['county']!='510')]\n",
    "clean_income_data = clean_income_data.loc[(clean_income_data['value']>=0)]\n",
    "bin_column= 'variable_name'\n",
    "sort_column = 'variable_code'\n",
    "count_column = 'value'\n",
    "category_field = 'variable_category'\n",
    "category = 'Household Income'\n",
    "cumulative_sort_columns = ['year_sample', 'sample_level', 'variable_code']\n",
    "median_income_basin = calculate_median_value(clean_income_data, bin_column, sort_column, count_column, category_field, category,grouping_variables, cumulative_sort_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_across_levels(df, bin_column, sort_column, count_column, category_field, category,grouping_variables, cumulative_sorting_variables):\n",
    "    basin_sort_columns = cumulative_sorting_variables\n",
    "    basin_grouping_variables = grouping_variables\n",
    "    county_sort_columns = ['county_name'] + cumulative_sorting_variables\n",
    "    county_grouping_variables = grouping_variables + ['county_name']\n",
    "    north_south_sort_columns = ['north_south'] + cumulative_sorting_variables\n",
    "    north_south_grouping_variables = grouping_variables + ['north_south']\n",
    "    basin_summary = calculate_median_value(df, bin_column, sort_column, count_column, category_field, category,grouping_variables, cumulative_sorting_variables)\n",
    "    county_summary = calculate_median_value(df, bin_column, sort_column, count_column, category_field, category, county_grouping_variables, county_sort_columns)\n",
    "    north_south_summary = calculate_median_value(df, bin_column, sort_column, count_column, category_field, category, north_south_grouping_variables, north_south_sort_columns)\n",
    "    basin_summary['Geography'] = 'Basin'\n",
    "    county_summary['Geography'] = county_summary['county_name']\n",
    "    north_south_summary['Geography'] = north_south_summary['north_south']\n",
    "    columns_to_keep = ['variable_code','variable_name', 'value', 'Geography', 'year_sample', 'dataset', 'sample_level', 'median_value']\n",
    "    basin_summary= basin_summary[columns_to_keep]\n",
    "    county_summary = county_summary[columns_to_keep]\n",
    "    north_south_summary = north_south_summary[columns_to_keep]\n",
    "    combined_summary = pd.concat([basin_summary, county_summary, north_south_summary], ignore_index=True)\n",
    "    combined_summary['Category'] = category\n",
    "    return combined_summary\n",
    "\n",
    "grouping_variables = ['variable_category', 'year_sample', 'dataset', 'variable_code', 'variable_name', 'sample_level']\n",
    "\n",
    "clean_income_data= all_data_clean.loc[(all_data_clean['county']!='510')&(all_data_clean['sample_level']=='tract')]\n",
    "\n",
    "clean_income_data = clean_income_data.loc[(clean_income_data['value']>=0)]\n",
    "bin_column= 'variable_name'\n",
    "sort_column = 'variable_code'\n",
    "count_column = 'value'\n",
    "category_field = 'variable_category'\n",
    "category = 'Household Income'\n",
    "cumulative_sort_columns = ['year_sample', 'variable_code', 'sample_level']\n",
    "median_incomes = median_across_levels(clean_income_data, bin_column, sort_column, count_column, category_field, category,grouping_variables, cumulative_sort_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_incomes.to_excel(\"Updated_Median_Incomes.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize non-Tahoe age data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import from an excel sheet because it's not worth bringing the detailed ones in\n",
    "dtypes = {\n",
    "    'GEO_ID' : str,\n",
    "'value' : int,\n",
    "'state' : str,\n",
    "'MarginOfError' : str,\n",
    "'county' : str,\n",
    "'sample_level' : str,\n",
    "'Geo_Name' : str,\n",
    "'variable_code' : str,\n",
    "'variable_name' : str,\n",
    "'variable_category' : str,\n",
    "'year_sample' : str,\n",
    "'dataset' : str,\n",
    "'census_geom_year' : str,\n",
    "'TRPAID' : str,\n",
    "}\n",
    "\n",
    "non_tahoe_age = pd.read_csv(\"non_tahoe_age_data.csv\", dtype= dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_34288\\4158647654.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  census_df['value'] = census_df['value'].astype(float)\n",
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_34288\\4158647654.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  census_df['value'] = census_df['value'].astype(float)\n",
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_34288\\4158647654.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  census_df['value'] = census_df['value'].astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GEO_ID', 'state', 'county', 'sample_level', 'Geo_Name', 'variable_category', 'year_sample', 'dataset', 'census_geom_year', 'TRPAID', 'Census Category with Sex']\n",
      "['GEO_ID', 'state', 'county', 'sample_level', 'Geo_Name', 'variable_category', 'year_sample', 'dataset', 'census_geom_year', 'TRPAID', 'Census Category with Sex']\n",
      "['GEO_ID', 'state', 'county', 'sample_level', 'Geo_Name', 'variable_category', 'year_sample', 'dataset', 'census_geom_year', 'TRPAID', 'Census Category with Sex']\n"
     ]
    }
   ],
   "source": [
    "dec_categories = 'Census_Category_Lists/dec_age_categories.csv'\n",
    "dec_categories_2020 = 'Census_Category_Lists/dec_age_categories_2020.csv'\n",
    "non_tahoe_age_2010 = non_tahoe_age.loc[non_tahoe_age['year_sample']=='2010']\n",
    "non_tahoe_age_2000 = non_tahoe_age.loc[non_tahoe_age['year_sample']=='2000']\n",
    "non_tahoe_age_2020 = non_tahoe_age.loc[non_tahoe_age['year_sample']=='2020']\n",
    "\n",
    "grouped_non_tahoe_age_2000 = categorize_values(non_tahoe_age_2000, dec_categories ,'Census Category with Sex', 'TRPA Census Age Sex Categories Grouped: ')\n",
    "grouped_non_tahoe_age_2010 = categorize_values(non_tahoe_age_2010, dec_categories ,'Census Category with Sex', 'TRPA Census Age Sex Categories Grouped: ')\n",
    "grouped_non_tahoe_age_2020 = categorize_values(non_tahoe_age_2020, dec_categories_2020 ,'Census Category with Sex', 'TRPA Census Age Sex Categories Grouped: ')\n",
    "\n",
    "grouped_non_tahoe_age = pd.concat([grouped_non_tahoe_age_2000, grouped_non_tahoe_age_2010, grouped_non_tahoe_age_2020], ignore_index=True)\n",
    "grouped_non_tahoe_age.to_excel('Census_Data_Downloads\\grouped_non_tahoe_age.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_16156\\4158647654.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  census_df['value'] = census_df['value'].astype(float)\n",
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_16156\\4158647654.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  census_df['value'] = census_df['value'].astype(float)\n",
      "C:\\Users\\amcclary\\AppData\\Local\\Temp\\ipykernel_16156\\4158647654.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  census_df['value'] = census_df['value'].astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GEO_ID', 'state', 'county', 'sample_level', 'Geo_Name', 'variable_category', 'year_sample', 'dataset', 'census_geom_year', 'TRPAID', 'Census Category']\n",
      "['GEO_ID', 'state', 'county', 'sample_level', 'Geo_Name', 'variable_category', 'year_sample', 'dataset', 'census_geom_year', 'TRPAID', 'Census Category']\n",
      "['GEO_ID', 'state', 'county', 'sample_level', 'Geo_Name', 'variable_category', 'year_sample', 'dataset', 'census_geom_year', 'TRPAID', 'Census Category']\n"
     ]
    }
   ],
   "source": [
    "dec_categories = 'Census_Category_Lists/dec_age_categories.csv'\n",
    "dec_categories_2020 = 'Census_Category_Lists/dec_age_categories_2020.csv'\n",
    "non_tahoe_age_2010 = non_tahoe_age.loc[non_tahoe_age['year_sample']=='2010']\n",
    "non_tahoe_age_2000 = non_tahoe_age.loc[non_tahoe_age['year_sample']=='2000']\n",
    "non_tahoe_age_2020 = non_tahoe_age.loc[non_tahoe_age['year_sample']=='2020']\n",
    "\n",
    "grouped_non_tahoe_age_2000 = categorize_values(non_tahoe_age_2000, dec_categories ,'Census Category', 'TRPA Census Age Categories Grouped: ')\n",
    "grouped_non_tahoe_age_2010 = categorize_values(non_tahoe_age_2010, dec_categories ,'Census Category', 'TRPA Census Age Categories Grouped: ')\n",
    "grouped_non_tahoe_age_2020 = categorize_values(non_tahoe_age_2020, dec_categories_2020 ,'Census Category', 'TRPA Census Age Categories Grouped: ')\n",
    "\n",
    "grouped_non_tahoe_age = pd.concat([grouped_non_tahoe_age_2000, grouped_non_tahoe_age_2010, grouped_non_tahoe_age_2020], ignore_index=True)\n",
    "grouped_non_tahoe_age.to_excel('Census_Data_Downloads\\grouped_non_tahoe_age_no_sex.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
